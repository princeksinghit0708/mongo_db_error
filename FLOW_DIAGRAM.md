# Complete System Flow Structure

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ENTRY POINT                                   â”‚
â”‚  example_usage.py or main.py                                     â”‚
â”‚  - Configures MongoDB connection                                 â”‚
â”‚  - Sets up Gemini API key (optional)                             â”‚
â”‚  - Specifies collections to analyze                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ErrorAnalysisPipeline                               â”‚
â”‚              (main.py - ErrorAnalysisPipeline class)              â”‚
â”‚                                                                   â”‚
â”‚  Orchestrates the entire 7-step pipeline                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚              â”‚              â”‚
        â–¼              â–¼              â–¼
```

## ğŸ“Š Complete Flow Diagram

### STEP 1: MongoDB Connection
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MongoDBConnector                   â”‚
â”‚  (mongodb_connector.py)             â”‚
â”‚                                     â”‚
â”‚  â€¢ connect()                        â”‚
â”‚    - Establishes MongoDB connection â”‚
â”‚    - Tests connection with ping      â”‚
â”‚    - Returns True/False             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
```

### STEP 2: Data Extraction
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  read_multiple_collections()        â”‚
â”‚  or get_error_collections()         â”‚
â”‚                                     â”‚
â”‚  For each collection:              â”‚
â”‚  1. read_collection()               â”‚
â”‚     - Queries MongoDB               â”‚
â”‚     - Converts to pandas DataFrame  â”‚
â”‚     - Normalizes nested structures   â”‚
â”‚       (event.header â†’ header_*)      â”‚
â”‚     - Maps errorCode â†’ errorType    â”‚
â”‚     - Handles timestamps            â”‚
â”‚                                     â”‚
â”‚  Returns:                           â”‚
â”‚  data_dict = {                      â”‚
â”‚    'abc': DataFrame,                 â”‚
â”‚    'cde': DataFrame,                â”‚
â”‚    ...                              â”‚
â”‚  }                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
```

### STEP 3: Error Pattern Analysis
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ErrorAnalyzer                       â”‚
â”‚  (error_analyzer.py)                 â”‚
â”‚                                     â”‚
â”‚  __init__(data_dict):               â”‚
â”‚    â€¢ Receives data_dict             â”‚
â”‚    â€¢ Combines all DataFrames        â”‚
â”‚      into self.combined_df          â”‚
â”‚                                     â”‚
â”‚  get_error_patterns():              â”‚
â”‚    â€¢ Error type frequency           â”‚
â”‚    â€¢ Collection distribution        â”‚
â”‚    â€¢ Temporal patterns (daily/hourly)â”‚
â”‚    â€¢ Raw data length stats          â”‚
â”‚    â€¢ Transaction amount stats       â”‚
â”‚                                     â”‚
â”‚  get_summary_statistics():          â”‚
â”‚    â€¢ Total records                  â”‚
â”‚    â€¢ Unique error types             â”‚
â”‚    â€¢ Date ranges                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
```

### STEP 4: Predictive Analytics
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PredictiveAnalytics                 â”‚
â”‚  (predictive_analytics.py)          â”‚
â”‚                                     â”‚
â”‚  A. Feature Engineering:            â”‚
â”‚     prepare_features():              â”‚
â”‚       â€¢ Temporal: hour, day, month  â”‚
â”‚       â€¢ Raw data: length, type       â”‚
â”‚       â€¢ Transaction: amount, log     â”‚
â”‚       â€¢ Categorical encoding         â”‚
â”‚                                     â”‚
â”‚  B. ML Model Training:              â”‚
â”‚     train_error_prediction_model():  â”‚
â”‚       â€¢ Random Forest               â”‚
â”‚       â€¢ Gradient Boosting           â”‚
â”‚       â€¢ XGBoost                     â”‚
â”‚       â€¢ Returns accuracy & metrics  â”‚
â”‚                                     â”‚
â”‚  C. Frequency Analysis:             â”‚
â”‚     analyze_error_frequency_patterns()â”‚
â”‚       â€¢ Daily trends                â”‚
â”‚       â€¢ Most frequent errors        â”‚
â”‚                                     â”‚
â”‚  D. Future Predictions:              â”‚
â”‚     predict_future_errors():        â”‚
â”‚       â€¢ 7-day ahead predictions     â”‚
â”‚       â€¢ Based on historical patternsâ”‚
â”‚                                     â”‚
â”‚  E. Feature Importance:              â”‚
â”‚     get_feature_importance():       â”‚
â”‚       â€¢ Which features matter most  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
```

### STEP 5: LLM Analysis (Optional)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Gemini LLM Integration              â”‚
â”‚  (predictive_analytics.py)          â”‚
â”‚                                     â”‚
â”‚  predict_error_reason_llm():        â”‚
â”‚    1. Gets prompt from              â”‚
â”‚       llm_prompts.py                â”‚
â”‚    2. Formats error record          â”‚
â”‚    3. Calls Gemini API              â”‚
â”‚    4. Returns analysis:             â”‚
â”‚       â€¢ Error reason                â”‚
â”‚       â€¢ Root causes                 â”‚
â”‚       â€¢ Recommendations            â”‚
â”‚                                     â”‚
â”‚  Analyzes sample of each            â”‚
â”‚  error type (max 3 for cost)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
```

### STEP 6: Visualization
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ErrorVisualizer                     â”‚
â”‚  (visualizer.py)                    â”‚
â”‚                                     â”‚
â”‚  Generates charts:                  â”‚
â”‚    â€¢ error_frequency.png            â”‚
â”‚    â€¢ temporal_trends.png            â”‚
â”‚    â€¢ collection_distribution.png    â”‚
â”‚    â€¢ model_performance.png           â”‚
â”‚    â€¢ feature_importance.png         â”‚
â”‚    â€¢ summary_dashboard.png          â”‚
â”‚                                     â”‚
â”‚  All saved to output/ directory     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
```

### STEP 7: Report Generation
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Report Generation                   â”‚
â”‚  (main.py - _generate_report())     â”‚
â”‚                                     â”‚
â”‚  Creates:                           â”‚
â”‚    â€¢ analysis_report.json           â”‚
â”‚      - Summary statistics           â”‚
â”‚      - Error patterns               â”‚
â”‚      - Model performance            â”‚
â”‚      - Future predictions           â”‚
â”‚      - LLM analysis                 â”‚
â”‚      - Recommendations             â”‚
â”‚                                     â”‚
â”‚    â€¢ analysis_report.txt            â”‚
â”‚      - Human-readable format        â”‚
â”‚                                     â”‚
â”‚  Saved to output/ directory         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ Complete Data Flow

```
MongoDB Database
    â”‚
    â”‚ (pymongo queries)
    â–¼
mongodb_connector.py
    â”‚
    â”‚ Returns: data_dict = {'collection_name': DataFrame}
    â–¼
error_analyzer.py
    â”‚
    â”‚ Combines â†’ combined_df (pandas DataFrame)
    â”‚ Analyzes patterns
    â–¼
predictive_analytics.py
    â”‚
    â”‚ Feature Engineering
    â”‚ ML Model Training
    â”‚ Future Predictions
    â”‚
    â”œâ”€â”€â–º LLM Analysis (Gemini API)
    â”‚
    â–¼
visualizer.py
    â”‚
    â”‚ Generates PNG charts
    â–¼
main.py (_generate_report)
    â”‚
    â”‚ Creates JSON & TXT reports
    â–¼
output/ directory
    â”‚
    â”œâ”€â”€ error_frequency.png
    â”œâ”€â”€ temporal_trends.png
    â”œâ”€â”€ collection_distribution.png
    â”œâ”€â”€ model_performance.png
    â”œâ”€â”€ feature_importance.png
    â”œâ”€â”€ summary_dashboard.png
    â”œâ”€â”€ analysis_report.json
    â””â”€â”€ analysis_report.txt
```

## ğŸ“ Module Responsibilities

### 1. **mongodb_connector.py**
   - **Purpose**: MongoDB data extraction
   - **Key Functions**:
     - `connect()`: Establishes MongoDB connection
     - `read_collection()`: Reads single collection
     - `read_multiple_collections()`: Reads multiple collections
     - `_normalize_nested_structure()`: Flattens nested MongoDB documents
   - **Output**: Dictionary of pandas DataFrames

### 2. **error_analyzer.py**
   - **Purpose**: Pattern analysis and statistics
   - **Key Functions**:
     - `get_error_type_frequency()`: Counts error occurrences
     - `get_temporal_analysis()`: Time-based patterns
     - `get_error_patterns()`: Comprehensive pattern analysis
   - **Output**: Analysis dictionaries and DataFrames

### 3. **predictive_analytics.py**
   - **Purpose**: ML predictions and LLM analysis
   - **Key Functions**:
     - `prepare_features()`: Feature engineering
     - `train_error_prediction_model()`: Trains 3 ML models
     - `predict_future_errors()`: 7-day predictions
     - `predict_error_reason_llm()`: Gemini LLM analysis
   - **Output**: Model results, predictions, LLM insights

### 4. **visualizer.py**
   - **Purpose**: Chart generation
   - **Key Functions**:
     - `plot_error_frequency()`: Bar charts
     - `plot_temporal_trends()`: Time series
     - `create_summary_dashboard()`: Combined dashboard
   - **Output**: PNG image files

### 5. **llm_prompts.py**
   - **Purpose**: LLM prompt templates
   - **Key Functions**:
     - `get_error_analysis_prompt()`: Main error analysis prompt
   - **Output**: Formatted prompt strings

### 6. **main.py**
   - **Purpose**: Pipeline orchestration
   - **Key Class**: `ErrorAnalysisPipeline`
   - **Orchestrates**: All 7 steps sequentially
   - **Output**: Complete analysis results

## ğŸ¯ Execution Flow Example

```python
# 1. User runs example_usage.py
pipeline = ErrorAnalysisPipeline(
    connection_string="mongodb://...",
    database_name="my_db",
    gemini_api_key="..."
)

# 2. Pipeline executes 7 steps:
pipeline.run_full_analysis(
    collection_names=['abc', 'cde'],
    limit=None
)

# Internal execution:
# Step 1: connector.connect() â†’ MongoDB connection
# Step 2: connector.read_multiple_collections() â†’ Data extraction
# Step 3: ErrorAnalyzer(data_dict) â†’ Pattern analysis
# Step 4: PredictiveAnalytics(df) â†’ ML & predictions
# Step 5: predictor.predict_error_reason_llm() â†’ LLM analysis
# Step 6: visualizer.plot_*() â†’ Generate charts
# Step 7: _generate_report() â†’ Create reports
```

## ğŸ”‘ Key Design Patterns

1. **Separation of Concerns**: Each module has a single responsibility
2. **Data Pipeline**: Sequential processing with clear data flow
3. **Error Handling**: Try-except blocks at critical points
4. **Flexibility**: Supports multiple collection structures
5. **Modularity**: Easy to extend or modify individual components

## ğŸ“Š Data Transformations

```
MongoDB Documents (BSON)
    â†“
Pandas DataFrames
    â†“
Normalized DataFrames (flattened structures)
    â†“
Feature Engineered DataFrames (ML-ready)
    â†“
ML Model Predictions
    â†“
Visualizations & Reports
```

This architecture ensures:
- âœ… Clean separation of data, analysis, and presentation
- âœ… Easy to test individual components
- âœ… Scalable to handle more collections
- âœ… Extensible for new analysis types
